{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit2 Least Squares, Determinants and Eigenvalues\n",
    "[page link](http://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/least-squares-determinants-and-eigenvalues/)\n",
    "Each component of a vector in $R^n$ indicates a distance along one of the coordinate axes. This practice of dissecting avector into directional components is an important one. In particular, it leads to the \"least squares\" method of fitting curves to collections of data. This unit also introduces matrix eigenvalues and eigenvectors. Many calculations become simpler when working with a basis of eigenvectors.\n",
    "\n",
    "The determinant of a matrix is anumber characterizing that matrix. This value is useful for determining whether a matrix is singular, computing its inverse, and more.\n",
    "\n",
    "# L1: Orthogonal Vectors and Subspaces\n",
    "Vectors are easier to understand when they're described in terms of orthogonal bases. In addition, the Four Fundamental Subspaces are orthogonal to each other in pairs.\n",
    "\n",
    "If A is a rectangular matrix, Ax = b is often unsolvable. The matrix ATA will help us find a vector x̂ that comes as close as possible to solving Ax = b.\n",
    "### Read Section 4.1 in the textbook.\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2: Projections onto Subspaces\n",
    "\t\n",
    "We often want to find the line (or plane, or hyperplane) that best fits our data. This amounts to finding the best possible approximation to some unsolvable system of linear equations Ax = b. The algebra of finding these best fit solutions begins with the projection of a vector onto a subspace\n",
    "\n",
    "### Read Section 4.2 in the textbook.\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L3: Projection Matrices and Least Squares\n",
    "\t\n",
    "Linear regression is commonly used to fit a line to a collection of data. The method of least squares can be viewed as finding the projection of a vector. Linear algebra provides a powerful and efficient description of linear regression in terms of the matrix ATA.\n",
    "\n",
    "### Read Section 4.3 in the textbook.\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L4: Orthogonal Matrices and Gram-Schmidt\n",
    "\t\n",
    "Many calculations become simpler when performed using orthonormal vectors or othogonal matrices. In this session, we learn a procedure for converting any basis to an orthonormal one.\n",
    "\n",
    "### Read Section 4.4 in the textbook.\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L5: Properties of Determinants\n",
    "\t\n",
    "\t\n",
    "The determinant of a matrix is a single number which encodes a lot of information about the matrix. Three simple properties completely describe the determinant. In this lecture we also list seven more properties like detAB = (detA)(detB) that can be derived from the first three.\n",
    "\n",
    "### Read Section 5.1 in the textbook.\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L6: Determinant Formulas and Cofactors\n",
    "\t\n",
    "\t\n",
    "The determinant of a matrix is a single number which encodes a lot of information about the matrix. Three simple properties completely describe the determinant. In this lecture we also list seven more properties like detAB = (detA)(detB) that can be derived from the first three.\n",
    "\n",
    "### Read Section 5.2 in the textbook.\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L7: Cramer's Rule, Inverse Matrix and Volume\n",
    "\t\n",
    "Now we start to use the determinant. Understanding the cofactor formula allows us to show that A-1 = (1/detA)CT, where C is the matrix of cofactors of A. Combining this formula with the equation x = A-1b gives us Cramer's rule for solving Ax = b. Also, the absolute value of the determinant gives the volume of a box.\n",
    "\n",
    "### Read Section 5.3 in the textbook.\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L8: Eigenvalues and Eigenvectors\n",
    "\t\n",
    "If the product Ax points in the same direction as the vector x, we say that x is an eigenvector of A. Eigenvalues and eigenvectors describe what happens when a matrix is multiplied by a vector. In this session we learn how to find the eigenvalues and eigenvectors of a matrix.\n",
    "\n",
    "### Read Section 6.1 6.2 in the textbook.\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L9: Diagonalization and Powers of A\n",
    "\t\n",
    "If A has n independent eigenvectors, we can write A = SΛS−1, where Λ is a diagonal matrix containing the eigenvalues of A. This allows us to easily compute powers of A which in turn allows us to solve difference equations $u_k+1 = A*u_k.$\n",
    "\n",
    "### Read Section 6.2 in the textbook.\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L10: Differential Equations and exp(At)\n",
    "\t\n",
    "We can copy Taylor's series for ex to define eAt for a matrix A. If A is diagonalizable, we can use Λ to find the exact value of eAt. This allows us to solve systems of differential equations du / dt = Au the same way we solved equations like dy / dt = ky.\n",
    "\n",
    "\n",
    "\n",
    "### Read Section 6.3 in the textbook.\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L11: Markov Matrices; Fourier Series\n",
    "\n",
    "Like differential equations, Markov matrices describe changes over time. Once again, the eigenvalues and eigenvectors describe the long term behavior of the system. In this session we also learn about Fourier series, which describe periodic functions as points in an infinite dimensional vector space.\n",
    "\n",
    "\n",
    "### Read Section 8.3, 8.5 in the textbook.\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam review\n",
    "Unit II covered a lot of material, the heart of this course. To go beyond the explanations in the lecture video, try reading the lecture summary which outlines orthogonality and least squares, determinants, and eigenvalues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
